\documentclass[anon,12pt]{colt2024} % Anonymized submission
%\documentclass[final,12pt]{colt2024} % Include author names

% The following packages will be automatically loaded:
% amsmath, amssymb, natbib, graphicx, url, algorithm2e

\title[Short Title]{
  % EDIT: Encoder-Decoder Image Transformer
  % EDiT: A Novel Encoder-Decoder Approach to Image Transformers
  EDIT: An Encoder-Decoder Image Transformer to Mitigate the Attention Sink Phenomenon
  }
\usepackage{times}

\usepackage{pifont}

% Use \Name{Author Name} to specify the name.
% If the surname contains spaces, enclose the surname
% in braces, e.g. \Name{John {Smith Jones}} similarly
% if the name has a "von" part, e.g \Name{Jane {de Winter}}.
% If the first letter in the forenames is a diacritic
% enclose the diacritic in braces, e.g. \Name{{\'E}louise Smith}

% Two authors with the same address
% \coltauthor{\Name{Author Name1} \Email{abc@sample.com}\and
%  \Name{Author Name2} \Email{xyz@sample.com}\\
%  \addr Address}

% Three or more authors with the same address:
% \coltauthor{\Name{Author Name1} \Email{an1@sample.com}\\
%  \Name{Author Name2} \Email{an2@sample.com}\\
%  \Name{Author Name3} \Email{an3@sample.com}\\
%  \addr Address}

% Authors with different addresses:
\coltauthor{%
 \Name{Author Name1} \Email{abc@sample.com}\\
 \addr Address 1
 \AND
 \Name{Author Name2} \Email{xyz@sample.com}\\
 \addr Address 2%
}

\begin{document}

\maketitle

\begin{abstract}%
% The application of transformer architectures in computer vision has been extensively studied in both supervised learning, with models such as ViT and DeiT, and self-supervised learning, with models such as MAE and DINO. 
% However, these architectures typically generate hidden image representations using only an encoder. 
% In this paper, we introduce an Encoder-Decoder architecture that incorporates a decoder aligned with the encoder layer by layer. This design allows the decoder to extract information progressively from low-level to high-level representations, leveraging more fine-grained source information. 
% Our experiments demonstrate that the Encoder-Decoder image Transformer (EDiT) outperforms DeiT in image classification and surpasses MAE in various downstream tasks.

In this paper, we propose EDIT (Encoder-Decoder Image Transformer), a novel architecture designed to mitigate the attention sink phenomenon observed in Vision Transformer (ViT) models. 
Attention sink occurs when an excessive amount of attention is allocated to the [CLS] token, distorting the model's ability to effectively process image patches. 
To address this, our architecture introduces a layer-aligned encoder-decoder framework, where the encoder utilizes self-attention to process image patches, while the decoder uses cross-attention to focus on the [CLS] token. 
Unlike traditional encoder-decoder framework, where the decoder depends solely on high-level encoder representations, EDIT allows the decoder to extract information starting from low-level features, progressively refining the representation layer by layer. 
Our results demonstrate that this approach reduces model complexity, mitigates the attention sink phenomenon, and significantly improves performance in image classification tasks.

\end{abstract}

\begin{keywords}%
  attention sink; encoder-decoder transformer architecture; vision transformer; computer vision; %self distillation
\end{keywords}

\section{Introduction}

Transformer, introduced by Vaswani et al. \cite{vaswani2017attention}, utilize self-attention and cross-attention mechanisms to extract intrinsic features from text data. 
Transformer includes both an encoder and a decoder, with the encoder extracting relevant information from input data and the decoder generating outputs based on this representation.
Transformer and its improvements have achieved significant success in natural language processing (NLP) tasks \cite{vaswani2017attention, devlin2018bert,radford2018improving, brown2020language,ouyang2022training}.

Improvements to the transformer architecture have led to two main branches: First, decoder-only models, like the GPT series \cite{radford2018improving, brown2020language,ouyang2022training}, are suited for generative tasks. Second, encoder-only models, such as BERT  \cite{devlin2018bert}, are tailored for classification tasks.

The success of transformers in NLP inspired researchers to explore their application in computer vision (CV), leading to the development of Vision Transformer (ViT) \cite{dosovitskiy2021an}, which directly processes sequences of image patches for classification tasks.
ViT can be viewed as an adaptation of the BERT model, a decoder-only model, for computer vision.
It transforms image patches and positions into a sequence of input tokens, then adds a special [CLS] token at the beginning of the sequence to aggregate information for classification.

The ViT model employs self-attention to learn relationships among these tokens, thus, resulting in three types of attention:
First, self-attention among input tokens to representations image features. Second, cross-attention from the [CLS] token to input tokens to assess feature importance for classification. Third, attention from input tokens to the [CLS] token, which maybe unnecessary or even detrimental.
The integration of these attention types can introduce irrelevant information and increase model complexity, and may compel the model to optimize conflicting objectives: guiding self-attention among patches while summarizing information for the linear classifier.

One piece of evidence supporting our point of view is the attention sink phenomenon we found in ViT.
The phenomenon of attention sink was first found in transformer-based language models \cite{xiaoefficient} refers to the tendency for a disproportionate amount of attention to be allocated to the initial tokens in a sequence, regardless of their semantic relevance.
This occurs due to the SoftMax function in attention mechanisms, which requires attention scores to sum to one.
Consequently, when there is no strong match for the current query among earlier tokens, excess attention is often directed towards these initial tokens, making them act as 'sinks' for unused attention values.

Similar to the attention sink phenomenon observed in transformer-based language models, we have identified an analogous phenomenon in ViT models, as illustrated in Figure \ref{fig:attention-sink-1} and \ref{fig:attention-sink-2}. In these models, a disproportionate amount of attention is allocated to the [CLS] token compared to other input tokens.
This bias towards the [CLS] token arises from the mixture of input tokens and the [CLS] token within ViT models.
The blending of these tokens can lead to conflicting objectives during attention processing, ultimately affecting the model's ability to effectively represent image features for classification tasks. 

In order to address this mixture of input tokens and the [CLS] token, we refer to the original encoder-decoder architecture, and propose to explicitly separate the [CLS] token from the input tokens, use the original self-attention layers to capture the attention between input tokens, and introduce new cross-attention layers to extract the attention of the [CLS] token to the input tokens.
We argue that this explicit separation avoids the contradiction and complexity of the traditional [CLS] token in ViT, and improves performance.

\section{Related work}

\subsection{Vision Transformer (ViT)}
In ViT, an image $\mathbf{x} \in \mathbb{R} ^ {h \times w \times c}$ is split into a sequence of flattened patches $\mathbf{x} \in \mathbb{R} ^ {n \times (p^2 \cdot c)}$ where $c$ is the number of channels, $(h,w)$ is the resolution of the original image, $(p,p)$ is the resolution of each image patch, and $n=hw/p^2$ is the number of patches.
Each flattened patch is then linearly projected to a patch embedding to produce a sequence of patch embeddings $\mathbf{x}_p \in \mathbb{R} ^ {n \times d}$ where $d$ is the embedded dimension size (or model dimension size).
To capture positional information, learnable position embeddings $\mathbf{pos} \in \mathbb{R} ^ {n \times d}$ are added to the sequence of patch embeddings to get the resulting sequence of patch and position embeddings $\mathbf{p}_0 = \mathbf{x}_p + \mathbf{pos}$.
After that, a learnable class embedding $\mathbf{c}_0 \in \mathbb{R}^d$ is concatenated to the sequence of patch and position embedding to produce the input sequence of tokens $\mathbf{z}_0 = \mathrm{concat}(\mathbf{p}_0,  \mathbf{c}_0) \in \mathbb{R} ^ {(n+1) \times d}$ for the ViT backbone.

The ViT backbone composed of $l$ layers is applied to the sequence of tokens $\mathbf{z}_0$ to generate a sequence of encodings $\mathbf{z}_l \in \mathbb{R} ^ {(n+1) \times d}$.
A ViT layer consists of a multi-head self-attention (MSA) block followed by a feed-forward network (FFN) block with layer norm (LN) applied before every block and residual connections added after every block:
\begin{eqnarray}
  \mathbf{a_{i-1}} &=& \mathbf{MSA}(\mathbf{LN}(\mathbf{z_{i-1}})) + \mathbf{z_{i-1}} , \\
  \mathbf{z_{i}} &=& \mathbf{FFN}(\mathbf{LN}(\mathbf{a_{i-1}})) + \mathbf{a_{i-1}},
\end{eqnarray}
where $i \in \{1,\ldots,l\}$.

The self-attention mechanism is composed of three point-wise linear layers mapping tokens to three intermediate representations, queries $\mathbf{q} \in \mathbb{R}^{(n+1) \times d}$, keys $\mathbf{k} \in \mathbb{R}^{(n+1) \times d}$ and values $\mathbf{v} \in \mathbb{R}^{(n+1) \times d}$.
Self-attention is then computed as follows:
\begin{equation}
   \mathbf{MSA}(\mathbf{q}, \mathbf{k}, \mathbf{v}) = \mathrm{softmax} \left( \frac{\mathbf{q}\mathbf{k}^T}{\sqrt{d}} \right)  \mathbf{v}.
\end{equation}

A FFN block is applied after the MSA block. It consists of two linear transformations and a nonlinear activation function within them, and can be denoted as the following function:
\begin{equation}
  \mathbf{FFN}(\mathbf{a}) = \mathbf{w_2} \cdot \sigma(\mathbf{w_1} \cdot \mathbf{a}),
\end{equation}
where $\mathbf{w_1}$ and $\mathbf{w_2}$ are two learnable matrices of the two linear transformations, $\sigma$ represents the nonlinear activation function, such as GELU, and  $\mathbf{a}$ is the output of the MSA block.

The output of ViT backbone $\mathbf{z}_l$ is feed into the ViT head, who first splits the generated encodings $\mathbf{z}_l$ into two partitions, the patch and position encodings $\mathbf{p}_l \in \mathbb{R} ^ {n \times d}$ and the class encoding $\mathbf{c}_l \in \mathbb{R}^d$, then extracts the class encoding.
After that, a linear transformation is applied to the class encoding $\mathbf{c}_l$ to produce a logits vector. A softmax layer is then used to transform the logits into class probabilities.

Researchers have recognized the problem of mixing input tokens with the [CLS]. In a later paper \cite{beyer2022better}, Beyer, one of the authors of the ViT model, have suggested that using 1D Global Average Pooling in place of the [CLS] token could enhance performance.
Touvron et al. \cite{touvron2021going} proposed to only allow the [CLS] token to attend to the patch tokens, rather than the opposite.

\subsection{Encoder-Decoder architecture}
The original transformer architecture, introduced by Vaswani et al. \cite{vaswani2017attention}, was designed for sequence-to-sequence tasks such as language translation, specially English-to-French and English-to-German.
It consists of two main components: an encoder and a decoder.
The encoder is responsible for processing and understanding the input text (source language). It converts the input sequence into a continuous representation, or embedding, which captures the relevant information from the source text. This embedding is then passed to the decoder for further processing.
The decoder takes the continuous representation from the encoder along with the previously generated translated tokens, and generates the output sequence (target language). It uses the encoder's output and the autoregressive property of the model to sequentially generate the translated text, predicting one token at a time based on the context provided by the encoder's embeddings and prior decoded tokens.
Together, the encoder and decoder form the basis of the transformer architecture, which enables effective handling of complex sequence-to-sequence tasks by leveraging self-attention and cross-attention mechanisms for capturing dependencies across tokens in the input and output sequences.

Recent advancements in encoder-decoder transformer models, such as BART \cite{lewis2020bart} and T5 \cite{raffel2020exploring,tay2023ul}, have significantly enhanced performance in various NLP tasks. These models combine innovative techniques, pre-training objectives, and architectural modifications to leverage the strengths of both the encoder and the decoder components.
Encoder-decoder models are particularly effective for tasks involving input and output sequences with varying lengths or structures. They excel in applications where it is crucial to model the relationships between elements in both sequences. Common use cases include text translation, summarization, question answering, and text generation. By capturing contextual information in the input sequence and generating a coherent output, these models have become central to a variety of NLP tasks.

% \subsection{Self-supervised learning}
% Recent advancements in self-supervised learning (SSL) \cite{he2022masked,wang2023image,caron2021emerging,oquab2024dinov} have increasingly utilized ViT as backbones, particularly in self-distillation models.
% A self-distillation model is a type of self-supervised learning framework in which a neural network (the student) is trained to improve its performance by learning from the predictions of its own earlier iteration or from another instance of itself (the teacher).
% In this process, the model utilizes its own outputs as pseudo-labels or guidance for training, rather than relying on external labels.

% DINO (Self-DIstillation with NO labels) \cite{caron2021emerging,oquab2024dinov} is a prominent self-distillation model that employs a ViT backbone to achieve state-of-the-art in self-supervised image representation learning. It utilizes a teacher-student framework where the student model learns from the teacher's outputs, which are themselves predictions derived from the same model. The teacher's parameters are freezed at the backpropagation stage, and then are updated from the student's parameters using exponential moving averages.



\section{Phenomenon of \textit{Attention sink} in ViT models}

To illustrate the phenomenon of attention sink, we downloaded two ViT-based image classification models \cite{touvron2021training} pre-trained on ImageNet-1k: deit\_small\_patch16\_224.fb\_in1k and deit\_base\_distilled\_patch16\_224.fb\_in1k, both obtained from the HuggingFace model repository.
We then extracted the attention maps from all 12 layers of these models and computed the amount of attention allocated to the [CLS] token, alongside the mean attention value directed towards the other input tokens. 
These results were then visualized in Figure \ref{fig:attention-sink-1} and \ref{fig:attention-sink-2} to effectively demonstrate the attention distribution across the layers and highlight the disproportionate focus on the [CLS] token.

This disproportionate focus on the [CLS] token suggests an over-centralization of attention resources on the classification token, potentially at the expense of reicher information integration from other input tokens.
This phenomenon could limit the model's ability to effectively leverage information from the entire input sequence, underscoring the need for explicitly separating the [CLS] token from the input tokens to mitigate attention sink.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.5\textwidth]{attention_sink.png}
  \caption{The phenomenon of attention sink in ViT models. The [CLS] token consistently receives much higher (more than 10 times) attention values, while attention directed toward other tokens remains relatively low and stable throughout the layers. }\label{fig:attention-sink-1}
\end{figure}
  
\begin{figure}[h!]
  \centering
  \includegraphics[width=0.5\textwidth]{attention_sink_2.png}
  \caption{Visualization of the attention scores across all 12 layers in deit\_small\_patch16\_224.fb\_in1k model. We visualize the starting $16\times16$ submatrix of the $197\times197$ attention maps of each layer. We observe that the model consistently focuses on the [CLS] token across all layers.}\label{fig:attention-sink-2}
\end{figure}
  

\section{Encoder-Decoder Image Transformer (EDIT)}

Our proposed architecture to mitigate attention sink is based on a novel encoder-decoder framework, where the encoder and decoder are aligned layer by layer.
Unlike the traditional encoder-decoder architecture, where the decoder typically relies on the high-level representations from the encoder, our architecture maintains a layer-wise alignment, enabling both components to interact more effectively at each stage of processing.

\subsection{Traditional Encoder-Decoder Architechture vs. Our Design}
In the traditional encoder-decoder architecture, the encoder processes the input tokens (source tokens) and generates a set of hidden representations, progressing from low-level to high-level features.
These representations are then passed to the decoder, which generates target token representations, typically starting from the highest-level representations of the input.

However, this design raises a few natural questions: Why should the low-level representations of target tokens depend solely on the highest-level representations of the source tokens? Why not allow each decoder layer to attend to the corresponding layer of the encoder, preserving more detailed information at each stage?

In contrast to this, our architecture avoids this high-level dependency by aligning the encoder and decoder layers.
Specially, each layer of the decoder is aligned with its corresponding encoder layer, ensuring that the encoder and decoder operate in tandem at the same level of abstraction.

\subsection{Layer-by-Layer Alignment}

In our design, the encoder consists of standard self-attention layers that process the input tokens (image pathes and position embeddings). The decoder, on the other hand, generates hidden representations for the [CLS] token by attending to the corresponding encoder layer and the previous decoder layer via a cross-attention mechanism.

The $i$-th layer of the encoder processes the $i$-th level of the input tokens, while the $i$-th layer of the decoder generates the target token representation based on both the source information from the $i$-th encoder layer and the previously generated target tokens from the $(i-1)$-th decoder layer. This design enables the decoder to extract information starting from low-level features, progressively refining the representation at each layer, rather than relying only on the high-level encoded information from the final encoder layer.

This layer-wise alignment has been shown to improve performance in tasks such as Neural Machine Translation (NMT), where precise alignment of encoder-decoder layers has been found to boost translation quality \cite{he2018layer}.

\subsection{Model Architechture} \label{subsection:model-arch}

The whole model architecture is illustrated in Figure \ref{fig:whole-arch}, which mainly includes two parts:
\begin{itemize}
  \item Tokenizer Block: The input image patches are first transformed into a sequence of tokens via the Tokenizer block.
  \item Backbone: The backbone of the model consists of $N$ identical layers, where each layer takes two parts: one from the encoder (tokens of image patches and position embeddings) and the other from the decoder ([CLS] token). These two inputs are processed in parallel, with outputs from the encoder and decoder feeding into the next layer. At the final layer, the output of the [CLS] token is passed through a linear transformation and a SoftMax layer to produce output probabilities.
\end{itemize}

Each layer of the model consists of two aligned sub-layers showed in Figure \ref{fig:layer-arch}:
\begin{itemize}
  \item Encoder Layer: This layer is identical to the standard ViT layer, which includes a multi-head self-attention mechanism followed by a fully connected feed-forward network. Both sub-layers are preceded by layer normalization, with residual connections around each of them.
  \item Decoder Layer: The decoder layer contains a cross-attention mechanism, which enables the decoder to attend to both the encoder's output and the previously generated decoder output. This cross-attention mechanism is followed by a layer normalization and a residual connection.
\end{itemize}

% \textbf{Weight Sharing in Decoder Layers.} 
It is important to note that the decoder layers in our model share weights, as indicated by the dashed outline in the diagram in Figure \ref{fig:whole-arch} and \ref{fig:layer-arch}. This weight-sharing strategy helps reduce model complexity and ensures consistent information flow across layers.

By aligning the encoder and decoder layers and incorporating cross-attention at each stage, our architecture improves the efficiency and effectiveness of information processing in ViT-based image classification tasks. The novel design ensures that the decoder has access to more fine-grained information, which helps in better feature extraction and more accurate classification, leading to improved overall model performance.


\begin{figure}[h!]
  \centering
  \includegraphics[width=0.5\textwidth]{arch.pdf}
  \caption{The whole architecture of EDIT model}\label{fig:whole-arch}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.5\textwidth]{arch2.pdf}
  \caption{The layer architecture of EDIT}\label{fig:layer-arch}
\end{figure}

\section{Experiments}

\subsection{Limitation of computational resources}
Due to the limitation of having only NVIDIA 3090 GPU cards, which do not support distributed training across nodes, we are restricted in training large models on extensive datasets. As a result, we have limited our largest model to EDIT-Base and constrained our largest dataset to a selected subset of ImageNet-21K. This restriction ensures that our experiments remain feasible within the computational resources available.

\subsection{Datasets} \label{subsec:datasets}
For pre-training, we use two large-scale image datasets.
First is \textbf{ImageNet-1k}, i.e. the ILSVRC-2012 ImageNet dataset \cite{deng2009imagenet}, containing 1,000 classes and approximately 1.3 million images.
Second is a selected subset of the ImageNet-21k dataset \cite{ridnik2021imagenetk,deng2009imagenet}.
% due to computational resource constraints. 
The original ImageNet-21k contains around 14 million images and 21,000 object categories. We first clean the dataset by removing infrequent classes with fewer than 500 images, resulting in 12,358,638 images across 11,221 classes. From this cleaned subset, we randomly select 6,000 classes, resulting in 6,633,989 images across 6,000 classes. We then allocate 50 images per class to create a standardized validation split. The resulting dataset are referred as \textbf{ImageNet-6k}.
%, retains about one-third of the original classes but only removes 43\% of the initial images.

For transfer learning evaluation, we use 4 popular computer vision datasets: CIFAR-100 \cite{krizhevsky2009learning}, Oxford Flowers-102 \cite{Nilsback08}, Describable Textures Dataset (DTD) \cite{cimpoi14describing}, and Caltech-256 \cite{griffin_holub_perona_2022}.

\subsection{Model variants}
We base the configurations of EDIT models on those used in DeiT3 \cite{touvron2022deit} and DeiT \cite{touvron2021training}, as detailed in Table \ref{tab:edit-variants}. The EDIT-Small and EDIT-Base models are adopted directly from DeiT3-Small and DeiT3-Base, while EDIT-Tiny integrates the original DeiT-Tiny model due to the absence of a DeiT3-Tiny configuration.
As described in subsection \ref{subsection:model-arch}, the EDIT models are constructed by adding iterative, weight-sharing decoder layers to the DeiT3 architecture. Given computational constraints, we limit our configurations to models no larger than DeiT3-Base.

\begin{table} [h!]
  \centering
  \begin{tabular}{l|rrrrr}
    \hline
    Model & Hidden size & Heads & Layers & Params & GFLOPs \\
    \hline
    EDIT-Tiny & 192 & 3 & 12 & 5.82M & 1.42 \\
    EDIT-Small & 384 & 6 & 12 & 22.30M & 5.28 \\
    EDIT-Base & 768 & 12 & 12 & 87.76M & 20.27 \\
    \hline
  \end{tabular}
  \caption{Variants of our EDIT architecture}
  \label{tab:edit-variants}
\end{table}

\begin{table} [h!]
  \centering
  \begin{tabular}{l|r|rr}
    \hline
    Methods & ImageNet-1k & \multicolumn{2}{c}{ImageNet-6k}  \\
     & & Pretrain & Transfer \\
    \hline
    Epochs & 800 & 100 & 400 \\
    Batch size & 2048 & 2048 & 768 \\
    Optimizer & LAMB & LAMB & LAMB \\
    Learning rate & $3.10^{-3}$ & $3.10^{-3}$ & $1.10^{-4}$ \\
    Learning rate decay & cosine & cosine & cosine \\
    Weight decay & 0.02 & 0.02 & 0.01 \\
    Warmup epochs & 5 & 5 & 5 \\
    \hline
    Label smoothing $\varepsilon$ & \ding{55} & 0.1 & 0.1 \\
    % Dropout &  &  &  \\
    Stoch. Depth & 0.05 & 0.05 & 0.1 \\
    Repeated Aug & \checkmark & \ding{55} & \ding{55} \\
    Gradient Clip & 1.0 & 1.0 & \ding{55} \\
    \hline
    H. flip & \checkmark & \checkmark & \ding{55} \\
    RRC & \checkmark  & \ding{55} & \ding{55} \\
    % Rand Augment &  &  &  \\
    3 Augment & \checkmark  & \checkmark  & \checkmark  \\
    LayerScale & \checkmark  & \checkmark  & \checkmark  \\
    Mixup alpha & 0.8 & \ding{55} & \ding{55} \\
    Cutmix alpha & 1.0 & 1.0 & 1.0 \\
    % Erasing prob. &  &  &  \\
    ColorJitter & 0.3 & 0.3 & \ding{55}  \\
    \hline
    Eval crop ratio & 1.0 & 1.0 & 1.0 \\
    \hline
    Loss & BCE & CE & CE \\
    \hline
  \end{tabular}
  \caption{Hyper-parameters of our models when trained on ImageNet-1k and ImageNet-6k, and transferred to CIFAR-100, Flowers-102, DTD and Caltech-256 datasets.}
  \label{tab:training-procedures}
\end{table}

\subsection{Training and transfer learning procedures}
We first train our models on ImageNet-1k and ImageNet-6k datasets, then transfer them to four downstream datasets: CIFAR-100, Flowers-102, DTD and Caltech-256.

Our training procedures follow those of DeiT3 with a few adjustments:
First, for the EDIT-Tiny model, the stochastic depth rate is reduced from 0.05 to zero, as suggested by the DeiT-Tiny configuration due to the lack of a DeiT3-Tiny model.
Second, due to computational constraints, the three EDIT models are trained for 100 epochs on the ImageNet-6k dataset, compared to the 240 epochs used for DeiT3 models on ImageNet-21k.
Last, due to computational constraints, we use gradient accumulation in the timm library \cite{rw2019timm} to reduce GPU memory requirements for the DeiT3-Base and EDIT-Base models. This adjustment results in a slight decrease in accuracy on ImageNet-1k, from 82.86 to 82.77, in our experiments.

For transfer learning, we reduce learning rate and batch size, adjust weight decay and stochastic depth rate, etc. to prevent overfitting on smaller datasets.
Table \ref{tab:training-procedures} indicates the hyper-parameters that we use by default for all our experiments.

\subsection{Comparison to DeiT3 model}

The comparison of our models to DeiT3 models is shown in Table \ref{tab:comparison-edit-deit3} and Figure \ref{fig:comparison-edit-deit3}.

As shown in Table \ref{tab:comparison-edit-deit3}, the EDIT models have a slight increase in parameter count parameter count and FLOPs compared to their DeiT3 counterparts, with EDIT-Tiny, EDIT-Small, and EDIT-Base showing a 2\%, 1\%, and 1\% increase in parameters, a 12\%, 14\%, and 15\% increase in FLOPs, respectively. This increase in computational cost is expected due to the iterative and weight-sharing decoder layers added in EDIT models.

On ImageNet-1k, we observe that the accuracy improvement of EDIT over DeiT3 diminishes as model size increase from Tiny to Small and then to Base.
This trend suggests that for a relatively smaller dataset like ImageNet-1k, larger models may encounter diminishing returns due to insufficient data to fully leverage the added capacity.
As model size increases without a corresponding increase in data volume or complexity, the model's capacity may become underutilized, resulting in smaller gains.

Conversely, on the larger ImageNet-6k dataset, the accuracy improvement of EDIT over DeiT3 grows as model size scales from Tiny to Small to Base.
This observation aligns with the neural scaling law \cite{Zhai_2022_CVPR,kaplan2020scaling}, as larger models can more effectively leverage the additional data and complexity provided by the ImageNet-6k dataset.
The increase data volume and diversity allow larger models to better generalize, thus enhancing the benefits of the EDIT architecture as model size increase.

An additional insight from these observation is that a mismatch between model size and data size can lead to suboptimal performance in either direction.
When a small model is paired with a large dataset, underfitting can occur, as the model lacks sufficient capacity to capture the full complexity of the data.
Conversely, a large model paired with a small dataset can lead to overfitting, as the model may focus on dataset-specific noise rather than learning robust, generalizable patterns. This underlines the importance of selecting a model size that aligns with the scale and diversity of the training data to achieve balanced and effective learning.

In summary, these results illustrate that while the EDIT models bring slight increase in computational cost, they generally offer accuracy improvements, especially on larger datasets like ImageNet-6k.
The scaling law in vision models is evident here: larger datasets effectively utilize the capacity of larger models, while smaller datasets may lead to overfitting in high-capacity architectures.


\begin{table} [h!]
  \centering
  \begin{tabular}{l|llll}
    \hline
    Models & Params & GFLOPs & ImageNet-1k  & ImageNet-6k  \\
     &  &  & top-1 (\%) & top-1 (\%) \\
    \hline
    EDIT-Tiny & 5.82M \small\textbf{($\uparrow$2\%)} & 1.42 \small\textbf{($\uparrow$12\%)} & 75.53 \small\textbf{($\uparrow$1.96)} & 49.02 \small\textbf{($\uparrow$0.25)} \\
    DeiT3-Tiny & 5.71M & 1.26 & 73.59 & 48.77 \\
    \hline
    EDIT-Small & 22.30M \small\textbf{($\uparrow$1\%)} & 5.28 \small\textbf{($\uparrow$14\%)} & 81.83 \small\textbf{($\uparrow$0.51)} & 54.48 \small\textbf{($\uparrow$0.47)} \\
    Deit3-Small & 22.05M & 4.61 & 81.32 & 54.01 \\
    \hline
    EDIT-Base & 87.76M \small\textbf{($\uparrow$1\%)} & 20.27 \small\textbf{($\uparrow$15\%)} & 82.70 \small\textbf{($\downarrow$0.07)} & 56.27 \small\textbf{($\uparrow$0.81)} \\
    Deit3-Base & 86.56M & 17.58 & 82.77 & 55.46 \\
    \hline
  \end{tabular}
  \caption{Comparison of parameters, throughput, and accuracy for EDIT and DeiT3 models when trained on (a) ImageNet-1k and (b) ImageNet-6k.}
  \label{tab:comparison-edit-deit3}
\end{table}


\begin{figure}[h]
  \centering
  \includegraphics[width=0.85\textwidth]{edit_deit3_compare.png}
  \caption{Comparison of top-1 accuracy between EDIT model and DeiT3 models when trained on ImageNet-1k and ImageNet-6k.}\label{fig:comparison-edit-deit3}
\end{figure}

\subsection{Transfer learning}
In order to evaluate the quality of the EDIT models learned through our training procedure we evaluated them with transfer learning tasks.
We focus on the performance of EDIT-Base model pre-trained on ImageNet-6k at resolution $224 \times 224$. 
The transfer learning procedures are also indicated in Table \ref{tab:training-procedures}.
Our results are presented in Table \ref{tab:transfer-learning}.
We observed that improved performance by 0.2\% on CIFAR-100, Flowers-102 and Caltech-256 datasets, and by 1\% on DTD dataset.

\begin{table} [h!]
  \centering
  \begin{tabular}{l|rrrr}
    \hline
    Model & CIFAR-100 & Flowers-102 & DTD  & Caltech-256  \\
    \hline
    EDIT-Base & \textbf{93.2} & \textbf{99.3} & \textbf{81.3} & \textbf{95} \\
    Deit3-Base & 93 & 99.2 & 80.3 & 94.8 \\
    \hline
  \end{tabular}
  \caption{Comparison between EDIT-Base and DeiT3-Base models on different transfer learning tasks with ImageNet-6k pre-training.}
  \label{tab:transfer-learning}
\end{table}



\subsection{Visualize attention maps}

\section{Conclusion}


% Acknowledgments---Will not appear in anonymized version
\acks{We thank a bunch of people and funding agency.}

\bibliography{custom}

\appendix

% \crefalias{section}{appendix} % uncomment if you are using cleveref

\section{My Proof of Theorem 1}

This is a boring technical proof.

\section{My Proof of Theorem 2}

This is a complete version of a proof sketched in the main text.

\end{document}
