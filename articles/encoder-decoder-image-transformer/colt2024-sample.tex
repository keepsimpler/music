\documentclass[anon,12pt]{colt2024} % Anonymized submission
%\documentclass[final,12pt]{colt2024} % Include author names

% The following packages will be automatically loaded:
% amsmath, amssymb, natbib, graphicx, url, algorithm2e

\title[Short Title]{
  % EDIT: Encoder-Decoder Image Transformer
  EDiT: A Novel Encoder-Decoder Approach to Image Transformers
  }
\usepackage{times}
% Use \Name{Author Name} to specify the name.
% If the surname contains spaces, enclose the surname
% in braces, e.g. \Name{John {Smith Jones}} similarly
% if the name has a "von" part, e.g \Name{Jane {de Winter}}.
% If the first letter in the forenames is a diacritic
% enclose the diacritic in braces, e.g. \Name{{\'E}louise Smith}

% Two authors with the same address
% \coltauthor{\Name{Author Name1} \Email{abc@sample.com}\and
%  \Name{Author Name2} \Email{xyz@sample.com}\\
%  \addr Address}

% Three or more authors with the same address:
% \coltauthor{\Name{Author Name1} \Email{an1@sample.com}\\
%  \Name{Author Name2} \Email{an2@sample.com}\\
%  \Name{Author Name3} \Email{an3@sample.com}\\
%  \addr Address}

% Authors with different addresses:
\coltauthor{%
 \Name{Author Name1} \Email{abc@sample.com}\\
 \addr Address 1
 \AND
 \Name{Author Name2} \Email{xyz@sample.com}\\
 \addr Address 2%
}

\begin{document}

\maketitle

\begin{abstract}%
The application of transformer architectures in computer vision has been extensively studied in both supervised learning, with models such as ViT and DeiT, and self-supervised learning, with models such as MAE and DINO. 
However, these architectures typically generate hidden image representations using only an encoder. 
In this paper, we introduce an Encoder-Decoder architecture that incorporates a decoder aligned with the encoder layer by layer. This design allows the decoder to extract information progressively from low-level to high-level representations, leveraging more fine-grained source information. 
Our experiments demonstrate that the Encoder-Decoder image Transformer (EDiT) outperforms DeiT in image classification and surpasses MAE in various downstream tasks.

\end{abstract}

\begin{keywords}%
  attention sink; encoder-decoder transformer architecture; vision transformers; computer vision; %self distillation
\end{keywords}

\section{Introduction}

Transformer, introduced by Vaswani et al. \cite{vaswani2017attention}, utilize self-attention and cross-attention mechanisms to extract intrinsic features from text data. 
Transformer includes both an encoder and a decoder, with the encoder extracting relevant information from input data and the decoder generating outputs based on this representation.
Transformer and its improvements have achieved significant success in natural language processing (NLP) tasks \cite{vaswani2017attention, devlin2018bert,radford2018improving, brown2020language,ouyang2022training}.

Improvements to the transformer architecture have led to two main branches: First, decoder-only models, like the GPT series \cite{radford2018improving, brown2020language,ouyang2022training}, are suited for generative tasks. Second, encoder-only models, such as BERT  \cite{devlin2018bert}, are tailored for classification tasks.

The success of transformers in NLP inspired researchers to explore their application in computer vision (CV), leading to the development of Vision Transformer (ViT), which directly processes sequences of image patches for classification tasks.
ViT can be viewed as an adaptation of the BERT model, a decoder-only model, for computer vision.
It transforms image patches and positions into a sequence of input tokens, then adds a special [CLS] token at the beginning of the sequence to aggregate information for classification.

The ViT model employs self-attention to learn relationships among these tokens, thus, resulting in three types of attention:
First, self-attention among input tokens to representations image features. Second, cross-attention from the [CLS] token to input tokens to assess feature importance for classification. Third, attention from input tokens to the [CLS] token, which maybe unnecessary or even detrimental.
The integration of these attention types can introduce irrelevant information and increase model complexity, and may compel the model to optimize conflicting objectives: guiding self-attention among patches while summarizing information for the linear classifier.

One piece of evidence supporting our point of view is the attention sink phenomenon we found in ViT.
The phenomenon of attention sink was first found in transformer-based language models \cite{xiaoefficient} refers to the tendency for a disproportionate amount of attention to be allocated to the initial tokens in a sequence, regardless of their semantic relevance.
This occurs due to the SoftMax function in attention mechanisms, which requires attention scores to sum to one.
Consequently, when there is no strong match for the current query among earlier tokens, excess attention is often directed towards these initial tokens, making them act as 'sinks' for unused attention values.

Similar to the attention sink phenomenon observed in transformer-based language models, we have identified an analogous phenomenon in ViT models, as illustrated in Figure . In these models, a disproportionate amount of attention is allocated to the [CLS] token compared to other input tokens.
This bias towards the [CLS] token arises from the mixture of input tokens and the [CLS] token within ViT models.
The blending of these tokens can lead to conflicting objectives during attention processing, ultimately affecting the model's ability to effectively represent image features for classification tasks. 

In order to address this mixture of input tokens and the [CLS] token, we refer to the original encoder-decoder architecture, and propose to explicitly separate the [CLS] token from the input tokens, use the original self-attention layers to capture the attention between input tokens, and introduce new cross-attention layers to extract the attention of the [CLS] token to the input tokens.
We argue that this explicit separation avoids the contradiction and complexity of the traditional [CLS] token in ViT, and improves performance.

\section{Related work}

\subsection{Vision Transformer (ViT)}
In ViT, an image $\mathbf{x} \in \mathbb{R} ^ {h \times w \times c}$ is split into a sequence of flattened patches $\mathbf{x} \in \mathbb{R} ^ {n \times (p^2 \cdot c)}$ where $c$ is the number of channels, $(h,w)$ is the resolution of the original image, $(p,p)$ is the resolution of each image patch, and $n=hw/p^2$ is the number of patches.
Each flattened patch is then linearly projected to a patch embedding to produce a sequence of patch embeddings $\mathbf{x}_p \in \mathbb{R} ^ {n \times d}$ where $d$ is the embedded dimension size (or model dimension size).
To capture positional information, learnable position embeddings $\mathbf{pos} \in \mathbb{R} ^ {n \times d}$ are added to the sequence of patch embeddings to get the resulting sequence of patch and position embeddings $\mathbf{p}_0 = \mathbf{x}_p + \mathbf{pos}$.
After that, a learnable class embedding $\mathbf{c}_0 \in \mathbb{R}^d$ is concatenated to the sequence of patch and position embedding to produce the input sequence of tokens $\mathbf{z}_0 = \mathrm{concat}(\mathbf{p}_0,  \mathbf{c}_0) \in \mathbb{R} ^ {(n+1) \times d}$ for the ViT backbone.

The ViT backbone composed of $l$ layers is applied to the sequence of tokens $\mathbf{z}_0$ to generate a sequence of encodings $\mathbf{z}_l \in \mathbb{R} ^ {(n+1) \times d}$.
A ViT layer consists of a multi-head self-attention (MSA) block followed by a feed-forward network (FFN) block with layer norm (LN) applied before every block and residual connections added after every block:
\begin{eqnarray}
  \mathbf{a_{i-1}} &=& \mathbf{MSA}(\mathbf{LN}(\mathbf{z_{i-1}})) + \mathbf{z_{i-1}} , \\
  \mathbf{z_{i}} &=& \mathbf{FFN}(\mathbf{LN}(\mathbf{a_{i-1}})) + \mathbf{a_{i-1}},
\end{eqnarray}
where $i \in \{1,\ldots,l\}$.

The self-attention mechanism is composed of three point-wise linear layers mapping tokens to three intermediate representations, queries $\mathbf{q} \in \mathbb{R}^{(n+1) \times d}$, keys $\mathbf{k} \in \mathbb{R}^{(n+1) \times d}$ and values $\mathbf{v} \in \mathbb{R}^{(n+1) \times d}$.
Self-attention is then computed as follows:
\begin{equation}
   \mathbf{MSA}(\mathbf{q}, \mathbf{k}, \mathbf{v}) = \mathrm{softmax} \left( \frac{\mathbf{q}\mathbf{k}^T}{\sqrt{d}} \right)  \mathbf{v}.
\end{equation}

A FFN block is applied after the MSA block. It consists of two linear transformations and a nonlinear activation function within them, and can be denoted as the following function:
\begin{equation}
  \mathbf{FFN}(\mathbf{a}) = \mathbf{w_2} \cdot \sigma(\mathbf{w_1} \cdot \mathbf{a}),
\end{equation}
where $\mathbf{w_1}$ and $\mathbf{w_2}$ are two learnable matrices of the two linear transformations, $\sigma$ represents the nonlinear activation function, such as GELU, and  $\mathbf{a}$ is the output of the MSA block.

The output of ViT backbone $\mathbf{z}_l$ is feed into the ViT head, who first splits the generated encodings $\mathbf{z}_l$ into two partitions, the patch and position encodings $\mathbf{p}_l \in \mathbb{R} ^ {n \times d}$ and the class encoding $\mathbf{c}_l \in \mathbb{R}^d$, then extracts the class encoding.
After that, a linear transformation is applied to the class encoding $\mathbf{c}_l$ to produce a logits vector. A softmax layer is then used to transform the logits into class probabilities.

Researchers have recognized the problem of mixing input tokens with the [CLS]. In a later paper \cite{beyer2022better}, Beyer, one of the authors of the ViT model, have suggested that using 1D Global Average Pooling in place of the [CLS] token could enhance performance.
Touvron et al. \cite{touvron2021going} proposed to only allow the [CLS] token to attend to the patch tokens, rather than the opposite.

\subsection{Encoder-Decoder architecture}
The original transformer architecture, introduced by Vaswani et al. \cite{vaswani2017attention}, was designed for sequence-to-sequence tasks such as language translation, specially English-to-French and English-to-German.
It consists of two main components: an encoder and a decoder.
The encoder is responsible for processing and understanding the input text (source language). It converts the input sequence into a continuous representation, or embedding, which captures the relevant information from the source text. This embedding is then passed to the decoder for further processing.
The decoder takes the continuous representation from the encoder along with the previously generated translated tokens, and generates the output sequence (target language). It uses the encoder's output and the autoregressive property of the model to sequentially generate the translated text, predicting one token at a time based on the context provided by the encoder's embeddings and prior decoded tokens.
Together, the encoder and decoder form the basis of the transformer architecture, which enables effective handling of complex sequence-to-sequence tasks by leveraging self-attention and cross-attention mechanisms for capturing dependencies across tokens in the input and output sequences.

% \subsection{Self-supervised learning}
% Recent advancements in self-supervised learning (SSL) \cite{he2022masked,wang2023image,caron2021emerging,oquab2024dinov} have increasingly utilized ViT as backbones, particularly in self-distillation models.
% A self-distillation model is a type of self-supervised learning framework in which a neural network (the student) is trained to improve its performance by learning from the predictions of its own earlier iteration or from another instance of itself (the teacher).
% In this process, the model utilizes its own outputs as pseudo-labels or guidance for training, rather than relying on external labels.

% DINO (Self-DIstillation with NO labels) \cite{caron2021emerging,oquab2024dinov} is a prominent self-distillation model that employs a ViT backbone to achieve state-of-the-art in self-supervised image representation learning. It utilizes a teacher-student framework where the student model learns from the teacher's outputs, which are themselves predictions derived from the same model. The teacher's parameters are freezed at the backpropagation stage, and then are updated from the student's parameters using exponential moving averages.


\section{Our proposal}

\subsection{Phenomenon of \textit{Attention sink} in ViT models}

To illustrate the phenomenon of attention sink, we downloaded the two ViT-based image classification models \cite{touvron2021training} pre-trained on ImageNet-1k, as provided by the authors of the paper. We extracted the attention maps from all 12 layers and computed the amount of attention allocated to the [CLS] token, alongside the mean attention value directed towards the other input tokens. 
These results were then visualized in Figure to effectively demonstrate the attention distribution across the layers and highlight the disproportionate focus on the [CLS] token.



\subsection{Encoder-Decoder Image Transformer (EDIT)}

Our design of the architecture is a kind of encoder-decoder architecture, where the encoder involves self-attention between patches, and the decoder extract attention of the [CLS] token to the patch and position tokens.

But, our architecture is not like the traditional encoder-decoder architecture.
The traditional encoder-decoder architecture is typically designed as following:
The encoder takes the source tokens (patches and positions here) as inputs and generates a set of hidden representations for those tokens layer by layer, gradually from low level to high level. Then the decoder takes the last-layer (the highest level) representations from the encoder as inputs, generates hidden representations for target tokens ([CLS] token here) from low-level layers to high-level ones.
% We can see that the generation of hidden representations for target tokens, no matter high level or low level, are all based on the highest-level representations of the source tokens.

% Then questions come out naturally: Why should the low-level representation of target token based on the highest-level ones of source tokens? Why not attending each layer of the decoder to each corresponding layer of the encoder?

Instead, in our design of the architecture, the encoder and decoder are aligned layer by layer.
Therefore, the encoder and decoder of our model have the same number of layers, and the $i$-th layer in the decoder is aligned and coordinated with the $i$-th layer of the encoder.
The hidden representation of a source token in the $i$-th layer using the self-attention mechanism, and that of a target token in the $i$-th layers is generated from the hidden representation of all source tokens and preceding target tokens in the $(i-1)$-th layer using a cross-attention mechanism.
Consequently, the decoder can extract the information from the source targets starting from the low-level representation layer by layer, and leverage more fine-grained source information, instead of only using high-level representations outputted by the encoder. Such an approach has been shown to be effective for Neural Machine Translation (NMT) \cite{he2018layer}.

The architecture of our model is illustrated in Figure \ref{fig1}.
The left half of Figure \ref{fig1} is the whole architecture. The image patches are transformed into a sequence of tokens by the Tokenizer block. The sequence of tokens and the [CLS] token are separately as two inputs of the backbone. The backbone composed of $N$ identical layers. Each layer accept two inputs and produce two outputs as the inputs of the next layer. The [CLS] token output of the final layer enter into the Linear and Softmax block to produce the output probabilities.

The right half of Figure \ref{fig1} is the structure of each layer. Each layer has two aligned sub-layers, the encoder layer and the decoder layer. The encoder layer is exactly same with the layer of the ViT model, which further composed of two sub-layers. The first is a multi-head self-attention mechanism and the seconde is a simple fully connected feed-forward network, both preceded by a layer normalization. And a residual connection around each of the two sub-layers are employed. The decoder layer has a cross-attention mechanism to fuse the output of its aligned encoder layer and the output of the precede decoder layer. And a residual connection around the cross-attention are employed and  followed by a layer normalization. 

It need noted that the decoder layers share their weights, which is illustrated by the dashed outline in the  Figure \ref{fig1}.


\begin{figure}[h]
  \centering
  \includegraphics[width=0.45\textwidth]{arch.pdf}
  \includegraphics[width=0.45\textwidth]{arch2.pdf}
  \caption{The architecture of Encoder-Decoder Image Transformer (EDIT)}\label{fig1}
\end{figure}
  
% Acknowledgments---Will not appear in anonymized version
\acks{We thank a bunch of people and funding agency.}

\bibliography{custom}

\appendix

% \crefalias{section}{appendix} % uncomment if you are using cleveref

\section{My Proof of Theorem 1}

This is a boring technical proof.

\section{My Proof of Theorem 2}

This is a complete version of a proof sketched in the main text.

\end{document}
